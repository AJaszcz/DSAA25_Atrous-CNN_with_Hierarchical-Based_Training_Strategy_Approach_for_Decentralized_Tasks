{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ca364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "DATA_ROOT = ''\n",
    "CLASS_NAMES = None\n",
    "\n",
    "# Image/Training Params\n",
    "IMG_SIZE = (150, 150)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS_BASELINE = 50\n",
    "\n",
    "# Hierarchical\n",
    "K_PROCESSES = 7\n",
    "T_LOCAL = 1\n",
    "M_DELETE_EVERY = 2 \n",
    "MAX_ROUNDS = 12\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260efb4b",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2575cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _list_images_in_class_dir(class_dir):\n",
    "    exts = ('*.png', '*.jpg', '*.jpeg', '*.bmp', '*.tif', '*.tiff')\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files.extend(glob.glob(os.path.join(class_dir, e)))\n",
    "    return files\n",
    "\n",
    "def _scan_classes(root, class_names=None):\n",
    "    if class_names is None:\n",
    "        classes = [d.name for d in Path(root).iterdir() if d.is_dir()]\n",
    "        classes.sort()\n",
    "    else:\n",
    "        classes = class_names\n",
    "    return classes\n",
    "\n",
    "def _load_paths_labels(root, class_names):\n",
    "    X, y = [], []\n",
    "    for idx, cname in enumerate(class_names):\n",
    "        cdir = os.path.join(root, cname)\n",
    "        files = _list_images_in_class_dir(cdir)\n",
    "        X.extend(files)\n",
    "        y.extend([idx]*len(files))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def _read_image(path, target_size):\n",
    "    img = tf.keras.utils.load_img(path, target_size=target_size)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "    return img\n",
    "\n",
    "def load_br35h(DATA_ROOT, class_names=None, img_size=(150,150), val_split=0.2, test_split=0.1):\n",
    "    data_root = Path(DATA_ROOT)\n",
    "    if not data_root.exists():\n",
    "        raise FileNotFoundError(f\"DATA_ROOT not found: {DATA_ROOT}\")\n",
    "\n",
    "    split_dirs = ['train', 'val', 'test']\n",
    "    if all((data_root / d).exists() for d in split_dirs):\n",
    "        tr_classes = _scan_classes(data_root / 'train', class_names)\n",
    "        X_train, y_train = _load_paths_labels(data_root / 'train', tr_classes)\n",
    "        X_val, y_val = _load_paths_labels(data_root / 'val', tr_classes)\n",
    "        X_test, y_test = _load_paths_labels(data_root / 'test', tr_classes)\n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), tr_classes\n",
    "\n",
    "    classes = _scan_classes(DATA_ROOT, class_names)\n",
    "    X, y = _load_paths_labels(DATA_ROOT, classes)\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(600 + 300), \n",
    "                                                        stratify=y, random_state=42, shuffle=True)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=300, \n",
    "                                                    stratify=y_temp, random_state=42, shuffle=True)\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), classes\n",
    "\n",
    "def make_tf_dataset(X_paths, y, batch_size=32, img_size=(150,150), shuffle=True, augment=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    def _load_and_preprocess(path, label):\n",
    "        img = tf.numpy_function(lambda p: _read_image(p.decode('utf-8'), img_size), [path], tf.float32)\n",
    "        img.set_shape(img_size + (3,))\n",
    "        img = img / 255.0\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "        return img, tf.one_hot(label, depth=NUM_CLASSES)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_paths.astype('U'), y.astype(np.int32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(1000, len(X_paths)), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), CLASS_NAMES_INFER = load_br35h(DATA_ROOT, CLASS_NAMES, IMG_SIZE)\n",
    "print('Classes:', CLASS_NAMES_INFER)\n",
    "print('Train/Val/Test sizes:', len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "train_ds = make_tf_dataset(X_train, y_train, BATCH_SIZE, IMG_SIZE, shuffle=True, augment=True)\n",
    "val_ds   = make_tf_dataset(X_val,   y_val,   BATCH_SIZE, IMG_SIZE, shuffle=False)\n",
    "test_ds  = make_tf_dataset(X_test,  y_test,  BATCH_SIZE, IMG_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc98a6",
   "metadata": {},
   "source": [
    "\n",
    "## ACBAM Model (Atrous-CNN with CBAM)\n",
    "\n",
    "- Input (150×150×3)\n",
    "- **Atrous Conv**: 32 filters, 3×3, dilation=2, ReLU → MaxPool 2×2\n",
    "- Three blocks (each: Conv 3×3 + MaxPool 2×2) with filters **64**, **128**, **128**\n",
    "- **Channel Attention**:\n",
    "  - Global **Max** and **Avg** pooling → Dense(16, ReLU) → Dense(128, ReLU), sum → **Sigmoid**\n",
    "  - Multiply with feature map\n",
    "- **Spatial Attention**:\n",
    "  - Channel-wise **Max** and **Avg** → concat → **Atrous Conv** 7×7, dilation=2, **Sigmoid**\n",
    "  - Multiply with feature map\n",
    "- Flatten → Dense(512, ReLU) → Dense(k, **Softmax**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def channel_attention(x, reduction_1=16, out_channels=128):\n",
    "    C = x.shape[-1]\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling2D()(x)\n",
    "    mlp = keras.Sequential([\n",
    "        layers.Dense(reduction_1, activation='relu'),\n",
    "        layers.Dense(out_channels, activation='relu')\n",
    "    ])\n",
    "    avg_out = mlp(avg_pool)\n",
    "    max_out = mlp(max_pool)\n",
    "    attn = layers.Activation('sigmoid')(layers.Add()([avg_out, max_out]))\n",
    "    attn = layers.Reshape((1,1,out_channels))(attn)\n",
    "    return layers.Multiply()([x, attn])\n",
    "\n",
    "def spatial_attention(x):\n",
    "    # x: (B, H, W, C)\n",
    "    avg_out = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    max_out = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "    concat = layers.Concatenate(axis=-1)([avg_out, max_out])\n",
    "    s = layers.Conv2D(1, kernel_size=7, dilation_rate=2, padding='same', activation='sigmoid')(concat)\n",
    "    return layers.Multiply()([x, s])\n",
    "\n",
    "def build_acbam_model(input_shape=(150,150,3), num_classes=2):\n",
    "    I = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', dilation_rate=2, activation='relu')(I)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "    for k in [64, 128, 128]:\n",
    "        x = layers.Conv2D(k, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    theta2 = x\n",
    "    theta3 = channel_attention(theta2, reduction_1=16, out_channels=128)\n",
    "    theta4 = theta3 \n",
    "    theta7 = spatial_attention(theta4)\n",
    "\n",
    "    flat = layers.Flatten()(theta7)\n",
    "    dense = layers.Dense(512, activation='relu')(flat)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(dense)\n",
    "        \n",
    "    model = keras.Model(inputs=I, outputs=outputs, name='ACBAM')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d374c",
   "metadata": {},
   "source": [
    "# Hierarchical strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebad69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, ds, y_true=None):\n",
    "    y_prob = model.predict(ds, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    if y_true is None:\n",
    "        y_true_list = []\n",
    "        for _, yb in ds:\n",
    "            y_true_list.extend(np.argmax(yb.numpy(), axis=1))\n",
    "        y_true = np.array(y_true_list)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='binary' if NUM_CLASSES==2 else 'macro', zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average='binary' if NUM_CLASSES==2 else 'macro', zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average='binary' if NUM_CLASSES==2 else 'macro', zero_division=0)\n",
    "    cm   = confusion_matrix(y_true, y_pred)\n",
    "    return {'acc':acc, 'precision':prec, 'recall':rec, 'f1':f1, 'cm':cm}, (y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6302db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_for_k_minus_2(X_train, y_train, k_processes, batch_size=32, img_size=(150,150)):\n",
    "    parts = k_processes - 2\n",
    "    idxs = np.arange(len(X_train))\n",
    "    np.random.shuffle(idxs)\n",
    "    splits = np.array_split(idxs, parts)\n",
    "    local_dsets = []\n",
    "    for s in splits:\n",
    "        Xp, yp = X_train[s], y_train[s]\n",
    "        ds = make_tf_dataset(Xp, yp, batch_size=batch_size, img_size=img_size, shuffle=True, augment=True)\n",
    "        local_dsets.append(ds)\n",
    "    return local_dsets\n",
    "\n",
    "def get_model_weights(model):\n",
    "    return model.get_weights()\n",
    "\n",
    "def set_model_weights(model, weights):\n",
    "    model.set_weights(weights)\n",
    "\n",
    "def average_weights_weighted(weight_list, coeffs):\n",
    "    avg = []\n",
    "    for weights in zip(*weight_list):\n",
    "        w = np.zeros_like(weights[0])\n",
    "        for wi, ci in zip(weights, coeffs):\n",
    "            w += wi * ci\n",
    "        avg.append(w)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ac6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg_with_removal(train_dsets, val_ds, test_ds, \n",
    "                        T_local=1, M_delete_every=2, max_rounds=12):\n",
    "    \n",
    "    num_clients = len(train_dsets)\n",
    "    clients = [build_acbam_model(IMG_SHAPE, NUM_CLASSES) for _ in range(num_clients)]\n",
    "    global_model = build_acbam_model(IMG_SHAPE, NUM_CLASSES)\n",
    "    \n",
    "    client_histories = {i: {\"train_acc\": [], \"val_acc\": []} for i in range(num_clients)}\n",
    "    history_records = {'global_val_acc':[], 'round':[]}\n",
    "\n",
    "    round_idx = 0\n",
    "    active = list(range(num_clients))\n",
    "    last_removed_ds = None\n",
    "    \n",
    "    while len(active) > 0 and round_idx < max_rounds:\n",
    "        for i in list(active):\n",
    "            set_model_weights(clients[i], global_model.get_weights())\n",
    "            hist = clients[i].fit(train_dsets[i], epochs=T_local, verbose=0, validation_data=val_ds)\n",
    "            client_histories[i][\"train_acc\"].append(hist.history[\"accuracy\"][-1])\n",
    "            client_histories[i][\"val_acc\"].append(hist.history[\"val_accuracy\"][-1])\n",
    "        \n",
    "        metrics = []\n",
    "        for i in active:\n",
    "            m, _ = evaluate_model(clients[i], val_ds)\n",
    "            metrics.append(m)\n",
    "\n",
    "        v_max = {\n",
    "            'acc': max(m['acc'] for m in metrics),\n",
    "            'precision': max(m['precision'] for m in metrics),\n",
    "            'recall': max(m['recall'] for m in metrics),\n",
    "            'f1': max(m['f1'] for m in metrics),\n",
    "        }\n",
    "        denom = 4.0\n",
    "        w_sig = []\n",
    "        for m in metrics:\n",
    "            score = (m['acc']/v_max['acc'] + m['precision']/v_max['precision'] + \n",
    "                     m['recall']/v_max['recall'] + m['f1']/v_max['f1'])/denom\n",
    "            w_sig.append(score)\n",
    "        w_sum = sum(w_sig)\n",
    "        coeffs = [w/w_sum for w in w_sig]\n",
    "        \n",
    "        weight_list = [get_model_weights(clients[i]) for i in active]\n",
    "        new_avg_weights = average_weights_weighted(weight_list, coeffs)\n",
    "        set_model_weights(global_model, new_avg_weights)\n",
    "\n",
    "        if (round_idx+1) % M_delete_every == 0 and len(active) > 0:\n",
    "            best_idx_in_active = int(np.argmin(w_sig))\n",
    "            to_remove = active[best_idx_in_active]\n",
    "\n",
    "            global_model.fit(train_dsets[to_remove], epochs=T_local, verbose=0, validation_data=val_ds)\n",
    "            last_removed_ds = train_dsets[to_remove]\n",
    "            active.remove(to_remove)\n",
    "        \n",
    "        val_eval = global_model.evaluate(val_ds, verbose=0)\n",
    "        history_records['global_val_acc'].append(val_eval[1])\n",
    "        history_records['round'].append(round_idx)\n",
    "        \n",
    "        round_idx += 1\n",
    "\n",
    "    if last_removed_ds is not None:\n",
    "        global_model.fit(last_removed_ds, epochs=T_local, verbose=0, validation_data=val_ds)\n",
    "    \n",
    "    return global_model, client_histories, history_records\n",
    "\n",
    "hier_model, client_histories, averaged_histories = fedavg_with_removal(\n",
    "    split_dataset_for_k_minus_2(X_train, y_train, K_PROCESSES, BATCH_SIZE, IMG_SIZE), val_ds, test_ds, T_local=T_LOCAL, \n",
    "    M_delete_every=M_DELETE_EVERY, max_rounds=MAX_ROUNDS\n",
    ")\n",
    "\n",
    "metrics_val_h, _ = evaluate_model(hier_model, val_ds)\n",
    "metrics_test_h, _ = evaluate_model(hier_model, test_ds)\n",
    "print('Hierarchical — Validation:', metrics_val_h)\n",
    "print('Hierarchical — Test:', metrics_test_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
