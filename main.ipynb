{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ca364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "DATA_ROOT = ''\n",
    "CLASS_NAMES = None\n",
    "\n",
    "# Image/Training Params\n",
    "IMG_SIZE = (150, 150)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "NUM_CLASSES = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260efb4b",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2575cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _list_images_in_class_dir(class_dir):\n",
    "    exts = ('*.png', '*.jpg', '*.jpeg', '*.bmp', '*.tif', '*.tiff')\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files.extend(glob.glob(os.path.join(class_dir, e)))\n",
    "    return files\n",
    "\n",
    "def _scan_classes(root, class_names=None):\n",
    "    if class_names is None:\n",
    "        classes = [d.name for d in Path(root).iterdir() if d.is_dir()]\n",
    "        classes.sort()\n",
    "    else:\n",
    "        classes = class_names\n",
    "    return classes\n",
    "\n",
    "def _load_paths_labels(root, class_names):\n",
    "    X, y = [], []\n",
    "    for idx, cname in enumerate(class_names):\n",
    "        cdir = os.path.join(root, cname)\n",
    "        files = _list_images_in_class_dir(cdir)\n",
    "        X.extend(files)\n",
    "        y.extend([idx]*len(files))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def _read_image(path, target_size):\n",
    "    img = tf.keras.utils.load_img(path, target_size=target_size)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "    return img\n",
    "\n",
    "def load_br35h(DATA_ROOT, class_names=None, img_size=(150,150), val_split=0.2, test_split=0.1):\n",
    "    data_root = Path(DATA_ROOT)\n",
    "    if not data_root.exists():\n",
    "        raise FileNotFoundError(f\"DATA_ROOT not found: {DATA_ROOT}\")\n",
    "\n",
    "    split_dirs = ['train', 'val', 'test']\n",
    "    if all((data_root / d).exists() for d in split_dirs):\n",
    "        tr_classes = _scan_classes(data_root / 'train', class_names)\n",
    "        X_train, y_train = _load_paths_labels(data_root / 'train', tr_classes)\n",
    "        X_val, y_val = _load_paths_labels(data_root / 'val', tr_classes)\n",
    "        X_test, y_test = _load_paths_labels(data_root / 'test', tr_classes)\n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), tr_classes\n",
    "\n",
    "    classes = _scan_classes(DATA_ROOT, class_names)\n",
    "    X, y = _load_paths_labels(DATA_ROOT, classes)\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(600 + 300), \n",
    "                                                        stratify=y, random_state=42, shuffle=True)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=300, \n",
    "                                                    stratify=y_temp, random_state=42, shuffle=True)\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), classes\n",
    "\n",
    "def make_tf_dataset(X_paths, y, batch_size=32, img_size=(150,150), shuffle=True, augment=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    def _load_and_preprocess(path, label):\n",
    "        img = tf.numpy_function(lambda p: _read_image(p.decode('utf-8'), img_size), [path], tf.float32)\n",
    "        img.set_shape(img_size + (3,))\n",
    "        img = img / 255.0\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "        return img, tf.one_hot(label, depth=NUM_CLASSES)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_paths.astype('U'), y.astype(np.int32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(1000, len(X_paths)), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), CLASS_NAMES_INFER = load_br35h(DATA_ROOT, CLASS_NAMES, IMG_SIZE)\n",
    "print('Classes:', CLASS_NAMES_INFER)\n",
    "print('Train/Val/Test sizes:', len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "train_ds = make_tf_dataset(X_train, y_train, BATCH_SIZE, IMG_SIZE, shuffle=True, augment=True)\n",
    "val_ds   = make_tf_dataset(X_val,   y_val,   BATCH_SIZE, IMG_SIZE, shuffle=False)\n",
    "test_ds  = make_tf_dataset(X_test,  y_test,  BATCH_SIZE, IMG_SIZE, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
